{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4DFtQNDYao1"
   },
   "source": [
    "# DynamicEmb Quick Start\n",
    "\n",
    "The primary goal of this notebook is to provide users with a fast introduction to and practical experience with the DynamicEmb API. The notebook walks through the process of creating an HKV embedding table with DynamicEmb and then training it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBgIy9eYYx35"
   },
   "source": [
    "## **Installation**\n",
    "Requirements:\n",
    "- TorchREC == v0.7\n",
    "\n",
    "DynamicEmb v0.1 has a dependency on our customized build of TorchREC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWBOrwVSnrNE"
   },
   "source": [
    "## **Overview**\n",
    "This tutorial offers a quick start guide to use DynamicEmb in TorchREC, covering the creation of an HKV embedding table and a sequential embedding lookup with both forward and backward operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udsN6PlUo1zF"
   },
   "source": [
    "### Torch Setup\n",
    "We setup our environment with torch.distributed and set our embedding config.\n",
    "\n",
    "Here, we use one rank corresponding to 1 GPU.\n",
    "\n",
    "**Bash Commands (Execute in a terminal):**\n",
    "Before run this notebook , you need set environment variable in your linux env\n",
    "```\n",
    "export RANK=0\n",
    "export WORLD_SIZE=1\n",
    "export MASTER_ADDR=127.0.0.1\n",
    "export MASTER_PORT=29500\n",
    "export LOCAL_WORLD_SIZE=1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4-v17rxkopQw",
    "outputId": "eb377d18-3fea-4b80-a220-97d1656c52f8"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "#Filter FBGEMM warning, make notebook clean\n",
    "warnings.filterwarnings(\"ignore\", message=\".*torch.library.impl_abstract.*\", category=FutureWarning)\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchrec\n",
    "import torch.distributed as dist\n",
    "backend = \"nccl\"\n",
    "dist.init_process_group(backend=backend)\n",
    "\n",
    "local_rank = dist.get_rank() #for one node\n",
    "world_size = dist.get_world_size()\n",
    "torch.cuda.set_device(local_rank)\n",
    "device = torch.device(f\"cuda:{local_rank}\")\n",
    "np.random.seed(1024+local_rank)\n",
    "               \n",
    "# Define the configuration parameters for the embedding table, \n",
    "# including its name, embedding dimension, total number of embeddings, and feature name.\n",
    "embedding_table_name = \"table_0\"\n",
    "embedding_table_dim = 128\n",
    "total_num_embedding = 1000\n",
    "embedding_feature_name = \"cate_0\"\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdSUWBRxoP8R"
   },
   "source": [
    "### Applying EmbeddingConfig and EmbeddingCollection in TorchREC. \n",
    "The conventions for defining embedding tables are unchanged from TorchREC. Users can utilize TorchREC's existing APIs for creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Iz_GZDp_oQ19"
   },
   "outputs": [],
   "source": [
    "eb_configs = [torchrec.EmbeddingConfig(\n",
    "                name=embedding_table_name,\n",
    "                embedding_dim=embedding_table_dim,\n",
    "                num_embeddings=total_num_embedding,\n",
    "                feature_names=[embedding_feature_name],\n",
    "            )]\n",
    "\n",
    "ebc = torchrec.EmbeddingCollection(\n",
    "        device=torch.device(\"meta\"),\n",
    "        tables=eb_configs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YLeBZoHwcFCL"
   },
   "source": [
    "# Configuring the DynamicEmb customized planner.\n",
    "\n",
    "Within DynamicEmb, the entry point for configuring and planning HKV embedding tables is the Customized Planner. DynamicEmb provides `DynamicEmbParameterConstraints`, `DynamicEmbeddingEnumerator`, and `DynamicEmbeddingShardingPlanner` to facilitate the creation of HKV embedding tables. Implemented either through inheritance or wrapping of the relevant TorchREC APIs, these APIs maintain both functional compatibility and a familiar usage pattern for TorchREC users.\n",
    "\n",
    "The following code is a simple example of apply a planner , this planner can plan for one HKV table. For a more detailed understanding of the DynamicEmb API usage, please refer to the API documentation provided in `DynamicEmb_APIs.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hrQHEtGpcFCL"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torchrec.distributed.comm import get_local_size\n",
    "from torchrec import DataType\n",
    "from torchrec.distributed.planner import EmbeddingShardingPlanner, Topology, ParameterConstraints\n",
    "from torchrec.distributed.types import (\n",
    "    ModuleSharder,\n",
    "    ShardingType,\n",
    ")\n",
    "from torchrec.distributed.planner.storage_reservations import (\n",
    "    HeuristicalStorageReservation,\n",
    ")\n",
    "from torchrec.distributed.types import (\n",
    "    BoundsCheckMode,\n",
    ")\n",
    "\n",
    "from dynamicemb.planner import  DynamicEmbParameterConstraints,DynamicEmbeddingShardingPlanner\n",
    "from dynamicemb.planner import  DynamicEmbeddingEnumerator\n",
    "from dynamicemb import DynamicEmbInitializerMode, DynamicEmbInitializerArgs, DynamicEmbTableOptions\n",
    "\n",
    "# use a function warp all the Planner code\n",
    "def get_planner(device, eb_configs,batch_size):\n",
    "    \n",
    "    DATA_TYPE_NUM_BITS: Dict[DataType, int] = {\n",
    "        DataType.FP32: 32,\n",
    "        DataType.FP16: 16,\n",
    "        DataType.BF16: 16,\n",
    "    }\n",
    "    \n",
    "    # For HVK  embedding table , need to calculate how many bytes of embedding vector store in GPU HBM\n",
    "    # In this case , we will put all the embedding vector into GPU HBM\n",
    "    eb_config = eb_configs[0]\n",
    "    dim = eb_config.embedding_dim\n",
    "    tmp_type = eb_config.data_type\n",
    "\n",
    "    embedding_type_bytes = DATA_TYPE_NUM_BITS[tmp_type]/8\n",
    "    emb_num_embeddings = eb_config.num_embeddings\n",
    "    emb_num_embeddings_next_power_of_2 = 2 ** math.ceil(math.log2(emb_num_embeddings)) # HKV need embedding vector num is power of 2\n",
    "    total_hbm_need = embedding_type_bytes*dim*emb_num_embeddings_next_power_of_2\n",
    "    \n",
    "    hbm_cap = 80 * 1024 * 1024 * 1024 # H100's HBM bytes per GPU\n",
    "    ddr_cap = 512 * 1024 * 1024 * 1024# Assume a Node have 512GB memory\n",
    "    intra_host_bw = 450e9 # Nvlink bandwidth\n",
    "    inter_host_bw = 25e9 # NIC bandwidth\n",
    "    \n",
    "    dict_const = {}\n",
    "\n",
    "    const = DynamicEmbParameterConstraints(\n",
    "            sharding_types=[\n",
    "                ShardingType.ROW_WISE.value,\n",
    "            ],\n",
    "            enforce_hbm=True,\n",
    "            bounds_check_mode=BoundsCheckMode.NONE,\n",
    "            use_dynamicemb=True,# from here , is all the HKV options , default use_dynamicemb is False , if it is False , it will fallback to raw TorchREC ParameterConstraints\n",
    "            dynamicemb_options = DynamicEmbTableOptions(\n",
    "                global_hbm_for_values=total_hbm_need,\n",
    "                initializer_args=DynamicEmbInitializerArgs(\n",
    "                    mode=DynamicEmbInitializerMode.NORMAL\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    dict_const[embedding_table_name] = const\n",
    "    topology=Topology(\n",
    "            local_world_size=get_local_size(),\n",
    "            world_size=dist.get_world_size(),\n",
    "            compute_device=device.type,\n",
    "            hbm_cap=hbm_cap,\n",
    "            ddr_cap=ddr_cap,  # For HVK  , if we need to put embedding vector into Host memory , it is important set ddr capacity\n",
    "            intra_host_bw=intra_host_bw,\n",
    "            inter_host_bw=inter_host_bw,\n",
    "        )\n",
    "        \n",
    "    # Same usage of  TorchREC's EmbeddingEnumerator\n",
    "    enumerator = DynamicEmbeddingEnumerator(\n",
    "                  topology = topology,\n",
    "                  constraints=dict_const,\n",
    "                )\n",
    "    \n",
    "    # Almost same usage of  TorchREC's EmbeddingShardingPlanner , but we need to input eb_configs, so we can plan every GPU's HKV object.\n",
    "    return DynamicEmbeddingShardingPlanner(\n",
    "            eb_configs = eb_configs,\n",
    "            topology = topology,\n",
    "            constraints=dict_const,\n",
    "            batch_size=batch_size,\n",
    "            enumerator=enumerator,\n",
    "            storage_reservation=HeuristicalStorageReservation(percentage=0.05),\n",
    "            debug=True,\n",
    "        )\n",
    "\n",
    "planner = get_planner(device, eb_configs, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7m0_ssVLFQEH"
   },
   "source": [
    "### Get a plan and Use TorchREC's DistributedModelParallel\n",
    "Now that we have successfully instantiated a DynamicEmb planner, the next step is to configure the optimizer and sharder. Then, using TorchREC's `DistributedModelParallel` function, we wrap the model into a DistributedModelParallel model.\n",
    "\n",
    "The basic process is identical to TorchREC, except when you want to use TorchREC's `EmbeddingCollectionSharder` function; in that case, please replace it with the `DynamicEmbeddingCollectionSharder` function. `DynamicEmbeddingCollectionSharder` overloads the dedup indexes process within TorchREC's `EmbeddingCollectionSharder` to accommodate HKV.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "arW0Jf6qEl-h",
    "outputId": "66c515f1-b432-4b8f-abca-40f346942fe4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DistributedModelParallel(\n",
      "  (_dmp_wrapped_module): ShardedDynamicEmbeddingCollection(\n",
      "    (lookups): \n",
      "     GroupedEmbeddingsLookup(\n",
      "        (_emb_modules): ModuleList(\n",
      "          (0): BatchedDynamicEmbedding(\n",
      "            (_emb_module): BatchedDynamicEmbeddingTables()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "     (_output_dists): \n",
      "     RwSequenceEmbeddingDist(\n",
      "        (_dist): SequenceEmbeddingsAllToAll()\n",
      "      )\n",
      "    (embeddings): ModuleDict(\n",
      "      (table_0): Module()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from fbgemm_gpu.split_embedding_configs import EmbOptimType\n",
    "from fbgemm_gpu.split_embedding_configs import SparseType\n",
    "\n",
    "from torchrec.distributed.fbgemm_qcomm_codec import get_qcomm_codecs_registry, QCommsConfig, CommType\n",
    "from torchrec.distributed.model_parallel import DefaultDataParallelWrapper, DistributedModelParallel\n",
    "\n",
    "from dynamicemb.shard import  DynamicEmbeddingCollectionSharder\n",
    "\n",
    "#set optimizer args\n",
    "learning_rate = 0.1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "weight_decay = 0\n",
    "eps = 0.001\n",
    "\n",
    "#Put args into a optimizer kwargs , which is same usage of TorchREC\n",
    "optimizer_kwargs = {\"optimizer\":EmbOptimType.ADAM ,\n",
    "                    \"learning_rate\": learning_rate,\n",
    "                    \"beta1\":beta1,\n",
    "                    \"beta2\":beta2,\n",
    "                    \"weight_decay\":weight_decay,\n",
    "                    \"eps\":eps}\n",
    "\n",
    "fused_params = {}\n",
    "fused_params[\"output_dtype\"] = SparseType.FP32\n",
    "fused_params.update(optimizer_kwargs)\n",
    "\n",
    "qcomm_codecs_registry = (\n",
    "        get_qcomm_codecs_registry(\n",
    "            qcomms_config=QCommsConfig(\n",
    "                # pyre-ignore\n",
    "                forward_precision= CommType.FP32,\n",
    "                # pyre-ignore\n",
    "                backward_precision= CommType.FP32,\n",
    "            )\n",
    "        )\n",
    "        if backend == \"nccl\"\n",
    "        else None\n",
    "    )    \n",
    "\n",
    "# Create a sharder , same usage with TorchREC , but need Use DynamicEmb function, because for index_dedup\n",
    "# DynamicEmb overload this process to fit HKV\n",
    "sharder = DynamicEmbeddingCollectionSharder(qcomm_codecs_registry=qcomm_codecs_registry,\n",
    "                                                    fused_params=fused_params, use_index_dedup=True)\n",
    "\n",
    "#Same usage of TorchREC\n",
    "plan = planner.collective_plan(ebc, [sharder], dist.GroupMember.WORLD)\n",
    "\n",
    "data_parallel_wrapper = DefaultDataParallelWrapper(\n",
    "        allreduce_comm_precision=\"fp16\"\n",
    "    )\n",
    "\n",
    "#Same usage of TorchREC\n",
    "model = DistributedModelParallel(\n",
    "        module=ebc,\n",
    "        device=device,\n",
    "        # pyre-ignore\n",
    "        sharders=[sharder],\n",
    "        plan=plan,\n",
    "        data_parallel_wrapper=data_parallel_wrapper,\n",
    "    )\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsxExmtBcFCM"
   },
   "source": [
    "## Generate data and begin forward and backward\n",
    "\n",
    "With the DistributedModelParallel model created, we can now train the embedding lookup. The code below demonstrates this by first generating the training data and then executing the forward and backward processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "5UITEgUccFCM",
    "outputId": "0b54a622-8f11-4cf1-e141-dc2d4fb9912e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter : 0 , cat_tensor = tensor([[-0.3054, -0.3133, -0.2628,  ...,  0.3496, -0.1596,  0.5477],\n",
      "        [-0.9145,  0.8816,  0.0056,  ...,  0.1427,  0.3048, -1.7920],\n",
      "        [-0.3354,  1.6754, -0.5813,  ..., -1.3018, -0.8106, -0.6762],\n",
      "        ...,\n",
      "        [ 2.2891, -1.5476,  0.6556,  ..., -1.6616, -0.3322,  0.5982],\n",
      "        [-1.3867,  1.6895,  1.3594,  ...,  1.2477, -1.1120, -0.4656],\n",
      "        [ 1.6933,  0.4228, -0.5796,  ..., -0.6652,  0.1855, -0.2718]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 1 , cat_tensor = tensor([[-1.3479,  1.5079, -0.7182,  ..., -0.4937, -0.6770,  0.8667],\n",
      "        [ 0.6271, -0.2502, -0.6837,  ...,  0.1481,  0.6616, -0.1672],\n",
      "        [-0.3730, -0.0913,  1.1698,  ..., -0.0586, -0.6082,  1.6253],\n",
      "        ...,\n",
      "        [ 0.9045,  0.3579,  1.3050,  ..., -0.2884,  0.2628,  1.4113],\n",
      "        [-0.6230,  0.9093,  1.6367,  ...,  1.5787, -0.1485, -0.4934],\n",
      "        [-2.1421,  0.1621, -0.7686,  ..., -1.1568, -0.3575, -1.2740]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 2 , cat_tensor = tensor([[ 0.4772,  0.7328, -0.8626,  ...,  0.9955,  0.0646,  0.3664],\n",
      "        [ 0.0313, -0.2783, -0.1938,  ...,  3.0435, -1.0681, -0.3421],\n",
      "        [-1.0405, -1.4538, -0.3084,  ..., -1.5362, -1.0928, -0.8218],\n",
      "        ...,\n",
      "        [ 0.0412, -0.3695, -0.6591,  ..., -0.3981,  0.9251,  0.9019],\n",
      "        [-1.9447,  0.1010,  1.1447,  ...,  1.1548,  0.7149,  1.2017],\n",
      "        [ 0.5950, -0.7364, -0.0677,  ..., -2.4027, -0.1538, -2.2456]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 3 , cat_tensor = tensor([[ 2.5921, -0.0687, -0.1907,  ...,  0.5075, -1.9904, -1.1571],\n",
      "        [-0.1449, -0.9676,  1.6271,  ..., -0.9189, -1.2681, -1.0138],\n",
      "        [ 0.4949, -1.3735, -1.5882,  ...,  0.9491,  1.3291,  0.9476],\n",
      "        ...,\n",
      "        [-0.0358,  1.4259,  0.2553,  ...,  1.2767, -0.9051,  0.4780],\n",
      "        [-0.9595, -1.2588, -0.2810,  ..., -0.0700,  0.4869,  1.8759],\n",
      "        [ 0.4652,  2.6977,  0.0645,  ..., -0.6998, -1.5163,  0.5293]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 4 , cat_tensor = tensor([[ 0.3421,  0.6340, -0.4719,  ...,  1.4054, -1.3627,  0.3642],\n",
      "        [-1.0689, -1.3266, -0.2849,  ..., -0.1053,  1.6514, -0.7874],\n",
      "        [ 2.7948, -1.5457,  0.1495,  ...,  1.7410,  0.8468,  2.0127],\n",
      "        ...,\n",
      "        [-0.4176,  0.3552,  0.7173,  ...,  0.2305,  0.0166,  0.0438],\n",
      "        [-1.1087,  0.8125,  1.4610,  ...,  1.9660, -0.5936, -0.8406],\n",
      "        [-1.1244,  0.1543, -0.0869,  ..., -0.9962,  1.2723, -0.7236]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 5 , cat_tensor = tensor([[-0.1802,  0.3795, -0.0480,  ..., -0.7607, -0.5207,  1.2806],\n",
      "        [ 0.9499, -0.3609, -0.1161,  ..., -0.5809, -0.3440, -0.3472],\n",
      "        [ 0.6952, -0.6151, -0.9424,  ..., -2.4135, -0.1942,  0.4737],\n",
      "        ...,\n",
      "        [-0.8330, -1.0677,  0.4319,  ...,  1.0364, -0.2438, -0.4524],\n",
      "        [ 0.2587,  0.0943,  0.2110,  ..., -0.8229, -0.0112,  0.0337],\n",
      "        [-0.7482,  1.9326, -0.8486,  ..., -0.0286,  0.2784, -0.8472]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 6 , cat_tensor = tensor([[ 0.2855,  1.0594, -1.2164,  ..., -0.1374, -0.6516, -2.4013],\n",
      "        [ 2.0840,  1.3254,  0.9616,  ...,  1.4752,  0.5916,  0.2720],\n",
      "        [-1.2404, -0.5213,  1.9194,  ..., -0.8521,  0.2538, -1.4844],\n",
      "        ...,\n",
      "        [ 0.7403,  0.2884, -2.2119,  ..., -0.8200,  0.8883, -0.6927],\n",
      "        [ 1.1063, -0.6877, -1.0865,  ..., -0.5827,  0.4804,  0.3212],\n",
      "        [-0.0565,  0.8298, -1.5307,  ...,  1.7474,  0.7968, -1.2943]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 7 , cat_tensor = tensor([[ 0.9145, -0.3089, -0.7889,  ...,  0.0853,  1.1251, -0.2241],\n",
      "        [-0.1929,  1.3482, -0.2665,  ...,  0.7162,  0.0521, -0.6970],\n",
      "        [-0.2504, -1.3222, -1.1884,  ..., -2.4488, -0.4685,  0.8481],\n",
      "        ...,\n",
      "        [ 0.0427,  0.3622,  0.8388,  ..., -1.9165, -1.4500, -0.0127],\n",
      "        [-1.0665,  0.4475, -0.1942,  ...,  0.0047,  0.4163,  0.5016],\n",
      "        [ 0.5210,  0.6398,  0.8721,  ...,  1.4661, -1.3312, -0.1745]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 8 , cat_tensor = tensor([[-0.0238,  0.2957,  0.7723,  ..., -1.9830, -1.5165, -0.0792],\n",
      "        [-1.2566, -1.6699, -0.5245,  ..., -1.7523, -1.3089, -1.0379],\n",
      "        [ 0.0313, -0.4986, -0.4791,  ..., -0.0543, -0.9822, -1.1926],\n",
      "        ...,\n",
      "        [ 1.0025,  0.3398, -0.6225,  ..., -1.0212, -0.2811, -0.1413],\n",
      "        [ 0.5250,  0.6282, -0.5975,  ..., -1.2251,  0.3289,  0.5637],\n",
      "        [ 0.3211,  0.1058,  1.3835,  ..., -0.1227,  0.3638, -0.0522]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n",
      "iter : 9 , cat_tensor = tensor([[ 0.3948,  1.7880, -0.3549,  ...,  0.2250,  1.0242,  0.2632],\n",
      "        [ 0.5788, -0.5116, -0.3987,  ...,  0.3196,  0.8859,  0.8368],\n",
      "        [-0.2864,  1.2871, -0.2452,  ..., -0.9945,  0.1088, -0.0385],\n",
      "        ...,\n",
      "        [-0.2703,  0.2148,  0.3264,  ...,  1.4648, -0.6796, -1.1492],\n",
      "        [ 1.1033, -0.3809,  0.6181,  ...,  0.6352,  1.0745,  0.9954],\n",
      "        [-0.6967,  0.0947,  0.5755,  ...,  1.7018,  1.5160, -0.3900]],\n",
      "       device='cuda:0', grad_fn=<CatBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_iterations = 10\n",
    "\n",
    "# This function generate a random indice to lookup\n",
    "def generate_sparse_feature(feature_num, num_embeddings_list, max_sequence_size,local_batch_size = 50):\n",
    "\n",
    "    prefix_sums = np.zeros(feature_num, dtype=int)\n",
    "    for f in range(1, feature_num):\n",
    "        prefix_sums[f] = prefix_sums[f - 1] + num_embeddings_list[f - 1]\n",
    "\n",
    "    indices = []\n",
    "    lengths = []\n",
    "\n",
    "    for f in range(feature_num):\n",
    "        unique_indices = np.random.choice(num_embeddings_list[f], size=(local_batch_size, max_sequence_size[f]), replace=True)\n",
    "        adjusted_indices = unique_indices\n",
    "        indices.extend(adjusted_indices.flatten())\n",
    "        lengths.extend([max_sequence_size[f]] * local_batch_size)\n",
    "\n",
    "    return torchrec.KeyedJaggedTensor(\n",
    "        keys=[f\"cate_{feature_idx}\" for feature_idx in range(feature_num)],\n",
    "        values=torch.tensor(indices, dtype=torch.int64).cuda(),\n",
    "        lengths=torch.tensor(lengths, dtype=torch.int64).cuda(),\n",
    "    )\n",
    "\n",
    "sparse_features = []\n",
    "for i in range(num_iterations):\n",
    "        sparse_features.append(generate_sparse_feature(feature_num = 1,\n",
    "                       num_embeddings_list=[total_num_embedding],\n",
    "                       max_sequence_size = [10],                              \n",
    "                       local_batch_size = batch_size // world_size))\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    sparse_feature = sparse_features[i]\n",
    "    ret = model(sparse_feature)\n",
    "\n",
    "    feature_names = []\n",
    "    tensors = []\n",
    "    for k, v in ret.items():\n",
    "        feature_names.append(k)\n",
    "        tensors.append(v.values())\n",
    "\n",
    "    cat_tensor = torch.cat(tensors, dim=1)\n",
    "    print(f\"iter : {i} , cat_tensor = {cat_tensor}\")\n",
    "    loss = cat_tensor.sum()\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebXfh7oW9fHH"
   },
   "source": [
    "## More resources\n",
    "For more information, please see DynamicEmb's `README.md` and `DynamicEmb_APIs.md` .\n",
    "\n",
    "If you want to compare raw TorchREC and DynamicEmb , please see benchmark folder's `README.md` , and test the benchmark in your node."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "custom": {
   "cells": [],
   "metadata": {
    "accelerator": "GPU",
    "colab": {
     "background_execution": "on",
     "collapsed_sections": [],
     "machine_shape": "hm",
     "name": "Torchrec Introduction.ipynb",
     "provenance": []
    },
    "fileHeader": "",
    "fileUid": "c9a29462-2509-4adb-a539-0318cf56bb00",
    "interpreter": {
     "hash": "d4204deb07d30e7517ec64733b2d65f24aff851b061e21418071854b06459363"
    },
    "isAdHoc": false,
    "kernelspec": {
     "display_name": "Python 3.7.13 ('torchrec': conda)",
     "language": "python",
     "name": "python3"
    },
    "language_info": {
     "codemirror_mode": {
      "name": "ipython",
      "version": 3
     },
     "file_extension": ".py",
     "mimetype": "text/x-python",
     "name": "python",
     "nbconvert_exporter": "python",
     "pygments_lexer": "ipython3",
     "version": "3.7.13"
    }
   },
   "nbformat": 4,
   "nbformat_minor": 0
  },
  "indentAmount": 2,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
